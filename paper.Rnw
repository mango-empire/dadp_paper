\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{statmath}
\usepackage{amsthm}
\usepackage{algpseudocodex}
\usepackage{algorithm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\newcommand{\dpn}{\textbf{dadp}}

\title{Data Augmentation for Privacy Aware Analysis}
\author{DP Group}
\date{September 2023}


\begin{document}

\maketitle

\begin{abstract}
    This paper serves as a reference and introduction on using the $\dpn$ R
    package. The goal of this package is to provide some tools for exploring the
    impact of different privacy regimes on a Bayesian analysis. A strength of
    this framework is the ability to target the exact posterior in settings
    where the likelihood is too complex to analytically express.
\end{abstract}

\section*{Methodology}
(Insert from DA paper?)



\section*{Using dadp}
Introduce some basic notation (here or in intro)
Consider combining this section with methodology.



Using the dadp package consist of specifying four components
\begin{enumerate}
  \item $\pi(\theta \mid x)$.
  \item $f(x \mid \theta)$.
  \item $\eta(s_{dp} \mid x)$.
  \item $T(x)$.
\end{enumerate}



\section*{Differentially Private Simple Linear Regression}

(Alabi et al. 2020)\cite{alabi2020} considers adding noise to a sufficient static
to create a differentially private algorithm for simple linear regression
called \textbf{NoisyStats}

\begin{algorithm}
\begin{algorithmic}[1]
\caption{NoisyStats: $(\epsilon, 0)$-DP Algorithm (closer to original paper)}
\State Data: $\{(x_i,y_i)\}_{i=1}^{n}$
\State Privacy Parameter: $\epsilon$
\State $\Delta_1 = \Delta_2 = (1 - 1/n)$ \Comment{Set global sensitivity}
\State Sample $L_1 \sim Lap(0, 3\Delta_1/\epsilon)$
\State Sample $L_2 \sim Lap(0, 3\Delta_2/\epsilon)$
\If{$nvar(x) + L_2 > 0$}
\State $\tilde{\beta} = \dfrac{ncov(x,y) + L_1}{nvar(x) + L_2}$\\
\State $\Delta_3 = (1/n)(1 + |\tilde{\alpha}|)$
\State Sample $L_3 \sim Lap(0, 3\Delta_3/\epsilon)$
\State $\tilde{\alpha} = (\bar{y} - \tilde{\beta}\bar{x}) + L_3$
\State \Return $(ncov(x,y) + L_1, nvar(x) + L_2, \tilde{\alpha})$
\EndIf
\State \Return NA
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\begin{algorithmic}[1]
\caption{NoisyStats: $(\epsilon, 0)$-DP Algorithm (easy for me!)}
\State Data: $\{(x_i,y_i)\}_{i=1}^{n}$
\State Privacy Parameter: $\epsilon$
\State $\Delta_1 = \Delta_2 = (1 - 1/n)$ \Comment{Set global sensitivity}
\State $\Delta_3 = \Delta_4 = 1/n$
\State Sample $L_1 \sim Lap(0, 3\Delta_1/\epsilon)$, $L_2 \sim Lap(0, 3\Delta_2/\epsilon)$
\State Sample $L_3 \sim Lap(0, 3\Delta_3/\epsilon)$, $L_4 \sim Lap(0, 3\Delta_4/\epsilon)$
\State $\tilde{s}_1 = ncov(x,y) + L_1$
\State $\tilde{s}_2 = nvar(x) + L_2$
\State $\tilde{s}_3 = \bar{y} + L_3$
\State $\tilde{s}_4 = \bar{x} + L_4$
\State \Return $(\tilde{s}_1,\tilde{s}_2,\tilde{s}_3,\tilde{s}_4)$
\end{algorithmic}
\end{algorithm}

Suppose we would like to explore the potential impact 
of the \textbf{NoisyStats} mechanism on analysis. Assume
the true data generating process is 
\begin{align*}
x_i &\sim Unif(0,1)\\
y_i &\sim N(-2 + 3 x_i, 3^2)
\end{align*}
We would like to perform inference on $(\alpha, \beta)$ 
given privatized statistic $(\tilde{s}_1, \tilde{s}_2)$.

\subsection*{Sampling from likelihood under complete data}
likelihood function $f(x) \sim Unif(0,1)$.
\begin{align*}
f(y \mid x,  \alpha, \beta) 
&= f(x \mid \mu_x, \sigma_x)f(y \mid x, \alpha, \beta)\\
&= \phi(x; \mu_x, \sigma_x)\phi(y; \alpha + \beta x, \sigma)
\end{align*}

<<echo = TRUE>>=
lik_smpl <- function(theta) {
  alpha <- theta[1]
  beta <- theta[2]
  x <- runif(1)
  y <- rnorm(1, mean = alpha + beta * x, sd = 3) 
  c(x,y)
}
@

\subsection*{Posterior given complete data}
Assume $f(\alpha,\beta) \sim N(0,10^{-2}I_{2 \times 2})$
\begin{align*}
\mu_p &= (1/9)\Sigma_p^{-1}X^Ty\\
\Sigma_p^{-1} &= (1/9)X^TX + (1/100)I^{-1} 
\end{align*}

<<echo = TRUE>>=
post_smpl <- function(dmat, theta) {
  x <- dmat[,1]
  y <- dmat[,2]
  xm <- cbind(1, x)
  Si <- (1/9) * t(xm) %*% xm + (1/100) * diag(2)
  mu <- (1/9) * solve(Si) %*% t(xm) %*% y
  MASS::mvrnorm(1, mu = mu, Sigma = solve(Si))
}
@


\subsection*{Statistic}
\textbf{NoisyStat} computes four summary statistics
\begin{align*}
nvar(x) &= \sum_{i=1}^{n}(x_i - \bar{x})\\
ncov(x,y) &= \sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})\\
\end{align*}
<<echo = TRUE>>=
st_calc <- function(dmat) {
  x <- dmat[,1]
  y <- dmat[,2]
  n <- length(y) - cov(x,y)/var(x)
  s1 <- (n-1) * cov(x,y)
  s2 <- (n-1) * var(x)
  s3 <- mean(y) 
  s4 <- mean(x)
  c(s1, s2, s3, s4)
}
@


\subsection*{Privacy Mechanism}
\textbf{NoisyStat} consist of adding independent Laplace errors
to each of the three summary statistics
<<echo = TRUE>>=
#check vectorization?
priv_mech_factory <- function(n, epsilon) {
  function(sdp, xt) {
    delta1 <- (1- 1/n)
    delta3 <- 1/n
    t1 <- VGAM::dlaplace(sdp[1] - xt[1], 0, 3 * delta1/epsilon, TRUE)
    t2 <- VGAM::dlaplace(sdp[2] - xt[2], 0, 3 * delta1/epsilon, TRUE)
    t3 <- VGAM::dlaplace(sdp[3] - xt[3], 0, 3 * delta3/epsilon, TRUE)
    t4 <- VGAM::dlaplace(sdp[4] - xt[4], 0, 3 * delta3/epsilon, TRUE)
    sum(c(t1,t2,t3,t4))
  }
}
@

\subsection*{Chain diagnostics?}
<<echo = FALSE, include = FALSE, cache = TRUE>>=
library(DPloglin)
set.seed(1)
epsilon <- 1
n <- 50
alpha <- 5
beta <- -3
x <- runif(n)
y <- alpha + beta*x + rnorm(n,0,3)
delta1 <- 1 - 1/n
delta3 <- 1/n
sdp <- st_calc(cbind(x,y))
sdp[1] <- sdp[1] + VGAM::rlaplace(1, 0, 3 * delta1/epsilon)
sdp[2] <- sdp[2] + VGAM::rlaplace(1, 0, 3 * delta1/epsilon)
sdp[3] <- sdp[3] + VGAM::rlaplace(1, 0, 3 * delta3/epsilon)
sdp[4] <- sdp[4] + VGAM::rlaplace(1, 0, 3 * delta3/epsilon)



dmod <- new_privacy(post_smpl = post_smpl,
                    lik_smpl = lik_smpl,
                    ll_priv_mech = priv_mech_factory(n, epsilon),
                    st_calc = st_calc,
                    add = FALSE,
                    npar = 2)

tmp <- mcmc_privacy(dmod,
               sdp = sdp,
               nobs = n,
               init_par = c(1,1),
               niter = 1000,
               chains = 1,
               varnames = c("alpha", "beta"))

@

<<echo = TRUE>>=
summary(tmp)
bayesplot::mcmc_trace(tmp$chain)
@

\newpage

\section*{A maybe...}


As an example, use data from (Gelman). Problem
consist of estimating the proportion of boys and girls.
Data: 251,527 boys and 241,945 girls born in Paris
from 1745 to 1770. Describe set up below

Privatize by adding noise, $\eta \sim N(0, 4000)$:
[Use DP framework?]
<<echo = TRUE>>=
n_g <- 241945
n_b <- 251527

eta <- rnorm(2,0,4000)

n_g + eta[1]
n_b + eta[2]
@

\subsection*{Sampling from likelihood under complete data}
binomial distribution
\begin{align*}
f(x \mid \theta) &= \binom{n}{n_g} \theta^{n_g}(1-\theta)^{n-n_g}
\end{align*}

<<echo = TRUE>>=
lik_smpl <- function(theta) {
  t1 <- rbinom(1, 493472, theta)
  t2 <- 493472 - t1
  c(t1,t2)
}
@

\subsection*{Posterior given complete data}
Using Jeffrey's prior $Beta(1/2,1/2)$.
\begin{align*}
\dfrac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}x^{\alpha -1}(1-x)^{\beta-1}
\end{align*}
conjugate model:




\nocite{*}
\bibliographystyle{amsplain}
\bibliography{ref.bib}



\end{document}
