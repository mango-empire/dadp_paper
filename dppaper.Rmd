---
title: "dapper: Data Augmentation for Private
Posterior Estimation in R"
date: "2023-11-01"
abstract: >
    This paper serves as a reference and introduction on using the dapper R
    package. It is an MCMC sampling framework which targets the
    exact posterior distribution given privitized data. 
    The goal of this package is to provide researchers a tool for exploring the
    impact of different privacy regimes on a Bayesian analysis. A strength of
    this framework is the ability to target the exact posterior in settings
    where the likelihood is too complex to analytically express.
draft: true
author:  
  - name: Kevin Eng
    affiliation: Rutgers University
    address:
    - Department of Statistics
    - Piscataway, NJ 08854
    url: https://www.britannica.com/animal/quokka
    email: ke157@stat.rutgers.edu
  - name: Jordan A. Awan
    affiliation: Purdue University
    address:
    - Department of Statistics
    - West Lafayette, IN 47907
    url: https://jordan-awan.com/
    email: jawan@purdue.edu
  - name: Ruobin Gong
    affiliation: Rutgers University
    address:
    - Department of Statistics
    - Piscataway, NJ 08854
    url: https://www.britannica.com/animal/quokka
    email: ruobin.gong@rutgers.edu
  - name: Nianqiao Phyllis Ju
    affiliation: Purdue University
    address:
    - Department of Statistics
    - West Lafayette, IN 47907
    url: https://www.britannica.com/animal/quokka
    email: nianqiao@purdue.edu
  - name: Vinayak A. Rao
    affiliation: Purdue University
    address:
    - Department of Statistics
    - West Lafayette, IN 47907
    url: https://www.britannica.com/animal/quokka
    email: varao@purdue.edu
type: package
output: 
  rjtools::rjournal_article:
    self_contained: yes
    toc: no
bibliography: RJreferences.bib

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, eval.after = "fig.cap")
library(ggplot2)
library(kableExtra)
library(tidyverse)
library(knitr)
```



# Introduction



Differential privacy provides a rigorous framework for protecting 
confidential information [@Dwork2006]. It has served as the theoretical foundation for recent advances in privacy technology. Several high profile
examples include Apple [@tang2017privacy], Google [@Erlingsson_2014], Microsoft [@ding2017collecting], and the 
U.S. Census Bureau [@Abowd2018].



Differential privacy methods provide plausible deniability by injecting a suitable amount of 
noise into a data analysis workflow. To account for the additional
noise from differential privacy, several different approaches
have been proposed. These approaches mainly differ in where in the workflow privacy noise is injected. 
Broadly speaking, noise can either be injected directly into a statistical estimating 
procedure or into the data itself. [@Ji2014]
provide a nice survey of how to modify several common machine learning algorithms
to be differentially private. The main drawback of this approach
is that it is model specific, so one needs to know before hand
what type of analysis is to be run. 


In the case where noise is directly injected into the data, one can
consider the *query* or *dissemination* scenario [@Hotz2022]. In the query setting,
the user is only allowed to make specific request for summary statistics. 
In the dissemination setting, full individual level micro data is released.
In both cases, privacy is maintained by adding noise directly to the released data product.
There is strong interest in further developing the query and dissemination approaches because data curators are often interested
in releasing data to the public without knowledge of how their data will
be used or analyzed. For a recent example of a high profile application of the dissemination approach
see the the U.S. Census Bureau's `TopDown` algorithm [@TopDown]. 

Correctly carrying out statistical inference in the dissemination setting
requires adjusting statistical workflows to account for privacy noise.
As an example, one instance of implementing the dissemination approach for tabular data involves directly
adding independent, random error to each cell. For regression models,
this corresponds to having data with measurement errors
in the covariates. This, unfortunately, violates the assumptions of most statistical models. 
In the presence of such errors, standard estimators can exhibit significant bias and incorrect uncertainty quantification 
[@Gong2022] [@karwa2015private] [@Wang2018].
These issues are a serious concern for researchers [@SantosLozada2020] [@Kenny2021] [@Winkler2021].
Therefore, developing privacy aware statistical workflows is necessary in order
for science and privacy to coexists.


Unfortunately, making the necessary adjustments poses formidable mathematical
challenges [@NIPS2010_sherry], even for seemingly simple models like linear regression. 
The difficulty can be seen from considering the marginal likelihood that results from correctly accounting for the injected
privacy noise. This function is often analytically intractable and as a result,
it is difficult or impossible to apply traditional statistical methods
to derive estimators. In particular, the marginal likelihood can involve a complex
integral where it is not possible to even evaluate the likelihood
at a point. And approximating the likelihood can be computationally
unfeasible since the integral is also of high dimension. 
Few tools are available to researchers to address these issues,
and their absence is a serious barrier to the wider adoption
of dissemination methods.

The `dapper` package provides a tool for conducting
valid statistical inference in the presence of privacy noise.
It implements the Bayesian framework proposed in @Ju2022. This framework describes how to modify
an existing Bayesian model to account for privacy noise. `dapper`
serves as a user-friendly interface for implementing the framework in R. It allows the
user to specify a sampler from an existing Bayesian model and
automatically constructing a valid posterior sampler that accounts for the 
added privacy noise. Compared to other Bayesian approaches, the `dapper` framework requires
no specialized knowledge for setting up samplers since there are no tuning parameters.
And when the record additivity property is satisfied (see section 6), the algorithmic run time 
complexity is similar to the non-private analysis. The goal of  `dapper` is to provide
a fast, flexible, and user-friendly way to analyze the impact of privacy mechanisms.


The rest of this article is organized as follows: Section 2 covers the necessary background to understand the mathematical notation
and ideas used throughout the paper. Section 3 goes over the main algorithm without
going into mathematical detail; for specifics see [@Ju2022]. Section 4 provides
an overview of the dapper package and discusses important implementation details.
Section 5 contains two examples of how one might use the package to analyze the 
impact of adding noise for privacy. The first example goes over a typical
odds ratio analysis for a $2 \times 2$ table and the second example
covers a linear regression model.

# Background

Let $x = (x_1, \ldots, x_n) \in \mathcal{X}^n$ represent a confidential 
database containing $n$ records. Usually, the goal of collecting data 
is to learn characteristics about the underlying population. 
To accomplish this task, a common approach is to assume the population
is represented by some statistical model $f( \cdot \mid \theta)$. It is often the case
some function of $\theta$ has relevant meaning to the scientific question at hand. In this setting,
learning characteristics of a population reduces to learning about $\theta$. 

In the Bayesian statistical framework, learning about $\theta$ is accomplished by drawing samples from the 
posterior $p(\theta \mid x) \propto f(x \mid \theta) p(\theta)$. 
Here, $p(\theta)$ is called the prior distribution, and represents
the researcher's belief about $\theta$ before seeing any data. 
One major advantage of the Bayesian method is that, through the prior, 
it provides a mechanism for incorporating information not explicitly contained
in the data at hand. This is especially useful in settings where there 
is considerable domain knowledge on the value of $\theta$.

For large data sets, it is common to work with a summary statistic $s = s(x)$
that has much smaller dimension than the original data. Doing so can
greatly simplify calculations. In general, there can be information
loss with using summary statistics, but for models with a sufficient 
statistic, there is no loss. Curators of large databases
often use summary statistics to publish data since it allows them
to efficiently communicate information contained in large data sets.
For this reason, summary statistics are a natural target for
dissemination based privacy approaches.


## Differential Privacy
While a summary statistic can already partially anonymize data, it is still 
possible to deduce information about an individual entry depending on
the distribution of $x$. Differential privacy
solves this problem by taking a summary statistic $s$, and adding noise to it to produce a noisy summary statistic $s_{dp}$.
While noise addition has
been common practice in statistical disclosure control [swapping citation], differential privacy
provides a rigorous framework to specify where and how much
noise to add. Most importantly, for the analyst, the differentially private noise mechanism is publicly
available and can be incorporated into subsequent analyses.


There are now many differential privacy frameworks, and the `dapper` method can be applied to
all of them. For presentation, we focus on the earliest and most common
formulation of differential privacy, $\epsilon$-differential privacy ($\epsilon$-DP). The
$\epsilon$ parameter is called the privacy loss budget. The privacy loss budget controls how 
strong the privacy guarantee is. Larger values of $\epsilon$ correspond to weaker
privacy guarantees which in turn means less noise being added.

We now describe the $\epsilon$-DP privacy framework in more detail. For the noisy summary 
statistic, we write $s_{dp} \sim \eta(\cdot \mid x)$. Here,
$\eta$ is a known noise infusion process designed to meet a certain property: The privacy mechanism
$\eta$ is said to be $\epsilon$-differentially private [@Dwork2006] if for all values of
$s_{dp}$, and all "neighboring" databases $(x,x') \in \mathcal{X}^n \times \mathcal{X}^n$ differing
by one record (denoted by $d(x,x') \leq 1$), the probability ratio is bounded:
\[
\dfrac{\eta(s_{dp} \mid x)}{\eta(s_{dp} \mid x')} \leq \exp(\epsilon), \quad \epsilon > 0.
\]

The differential privacy framework is used to create and verify privacy 
mechanisms. One such mechanism is the *Laplace mechanism*. It works by 
taking a deterministic statistic $s: X \mapsto \mathbb{R}^m$ and constructs
the privatized statistic $s_{dp} := s(x) + u$ where $u$ is a $m$-dimensional
vector of i.i.d. Laplace random variables. The amount of noise, $u$, is scaled
proportionally to the *global sensitivity* (or just sensitivity) of the statistic $s$. 
We define the sensitivity of a statistic $s$ as 
$\Delta (s) := \max_{(x,x') \in \mathcal{X}^n \times \mathcal{X}^n; d(x,x') \leq 1} \|s(x) - s(x')\|$.
Using the ratio bound, if we draw
each $u_i \sim \text{Lap}(\Delta (s) / \epsilon)$, we can show $s_{dp}$ is $\epsilon$-differentially private
for the the Laplace mechanism. Roughly speaking
global sensitivity can be thought of as quantifying how easy it is to identify
a particular record. The idea being the easier it is to identify a record (high global sensitivity),
the more noise that needs to be added to achieve a given privacy guarantee [@Dwork2006]. Example 2, will cover an
application of the Laplace mechanism to linear regression.


# Methodology
Given data privatized data, $s_{dp}$, the goal of Bayesian inference is to sample from the 
posterior distribution $p(\theta \mid s_{dp})$. Since the observed likelihood,
$p(s_{dp} \mid \theta)$, often has no simple closed form expression [@NIPS2010_sherry], most standard
sampling schemes do not apply. To conduct privacy-aware Bayesian inference, the dapper package implements
the data augmentation algorithm of @Ju2022 which allows us to sample from $p(\theta \mid s_{dp})$
without needing to specify $p(s_{dp} \mid \theta)$.

The algorithm considers the joint distribution $p(\theta, x \mid s_{dp})$ and
alternates sampling from the two distributions $p(\theta \mid x, s_{dp})$
and $p(x \mid \theta, s_{dp})$.

Since $s_{dp}$ is derived from $x$, we have $p(\theta \mid x, s_{dp}) = p(\theta \mid x)$ which
is just the usual posterior distribution given the confidential data $x$. The dapper
package assumes the user has access to a sampler for $p(\theta \mid x)$. This can
come from any R package such as \CRANpkg{fmcmc} or constructed analytically via posterior conjugacy.
For the second distribution, $p(x \mid \theta, s_{dp})$, may
only be known up to a constant. The dapper package samples from this distribution by
running a Gibbs-like sampler. Each of the $n$ components of $x$ is individually
updated. However unlike the standard Gibbs sampler, each component is updated
using a Metropolis-Hasting algorithm. This method is sometimes called the Metropolis within Gibbs sampler [@Robert2004].

In some cases, sampling from $p(x \mid \theta, s_{dp})$ can be made more efficient
when the privacy mechanism can be written as a function of $s_{dp}$ and
a sum consisting of contribution from each individual record. More precisely, we say the privacy mechanism satisfies
the *record additivity* property if 
\[
\eta(s_{dp} \mid x) = g\left(s_{dp}, \sum_{i=1}^{n}t_i(x_i, s_{dp}) \right)
\]
for some known and tractable functions $g, t_1, \ldots, t_n$. The sample mean is a
example of a summary statistic satisfying record additivity where $t_i(x_i, s_{dp}) = x_i$.


The data augmentation algorithm is described in the following pseudo code:

1. Sample $\theta^{t+1}$ from $p(\cdot \mid x^{(t)})$.
2. Sample from $p(x \mid \theta, s_{dp})$ using a three step process
    + Propose $x_{i}^{*} \sim f(\cdot \mid \theta)$.
    + If $s$ satisfies the record additive property then
    update $s(x^*, s_{dp}) = t(x,s_{dp}) - t_i(x_i,s_{dp}) + t_{i}(x_i^*, s_{dp})$.
    + Accept the proposed state with probability $\alpha(x_i^* \mid x_i, x_{-i}, \theta)$
    given by:
    
    \[
      \alpha(x_i^* \mid x_i, x_{-i}, \theta) = \min \left\{ 1, \dfrac{\eta(s_{dp} \mid s(x_i^*, x_{-i}))}{\eta(s_{dp} \mid s(x_i, x_{-i}))} \right\}  
      = \min \left\{ 1, \dfrac{g(s_{dp}, t(x^*, s_{dp}))}{g(s_{dp}, t(x,s_{dp}))} \right\}.
    \]
    
Theoretical results such as bounds on the acceptance probability and a proof
of geometric ergodicity can be found in [@Ju2022].


# The Structure of dapper

The `dapper` package is structured around the two functions `dapper_sample` and
`new_privacy`. The first function is used to draw samples from the 
posterior. The second function is used to create a privacy data model object
which the `dapper_sample` function requires as input. The purpose of the data model
object is to collect all the components specific to the data augmentation algorithm
into one bundle. This way, the other arguments into `dapper_sample` pertain only 
to sampling parameters such as the number of iterations.


Since the input to these functions are R functions, there is a great deal of freedom
in implementation. The next two sections describe in detail the inputs into
these functions and highlight some considerations that should be taken
into account in order to avoid slow or unexpected behavior.

Before delving into  the specifics of each component, it is necessary to clearly
define how the confidential data is represented. Internally, the 
confidential database is encoded as a 2D matrix. Each row
of this matrix must correspond to a record.


## Privacy Model

Creating a privacy model is done using the `new_privacy` constructor. The 
main arguments consist of the four components as outlined in the methodology
section.

```{r, echo=TRUE, eval=FALSE}
new_privacy(post_f = NULL, latent_f = NULL, priv_f = NULL,
            st_f = NULL, npar = NULL)
```

The internal implementation of the data augmentation algorithm in `dapper_sample` requires
some care in how each component is constructed. 

* `latent_f` is an R function that samples from the latent process.
The latent process, in this case, is the probability model which dictates how
to generate a new confidential record $x$, given $\theta$.
Its syntax should be `latent_f(theta)` where `theta` is a vector
representing the model parameters being estimated. This function
must work with the supplied initial parameter provide in the `init_par`
argument of `dapper_sample` function. The output is a $n \times p$ matrix
where $n$ is the number of observations and $p$ is the dimension of a record $x$. 



* `post_f` is a function which makes draws from the posterior sampler. It should
have the syntax `post_f(dmat, theta)`. Here `dmat` is the 
hypothetical data set representing the confidential data. This sampler can be generated by wrapping MCMC samplers generated from other R packages 
(e.g. \CRANpkg{rstan}, \CRANpkg{fmcmc}, \CRANpkg{adaptMCMC}).
If using this approach, it is recommended to avoid using packages 
with a large initialization overhead such as \CRANpkg{mcmc} since the sampler is reinitialized
every loop iteration. In the case of \CRANpkg{mcmc},
the Metropolis-Hastings loop is implemented in C so there is a significant initialization cost
when calling from an R function. The purpose of the `theta` argument is 
to serve as the initialization point if samples from `post_f` are draws
from say a Metropolis-Hastings sampler.

* `priv_f` is an R function that represents the log of the privacy mechanism density, $\eta(s_{sdp} \mid x)$. 
Typically this function has the form `priv_f(arg1, arg2)` where `arg1` is the
the value of $s_{dp}$ and `arg2` is the confidential summary statistic $s$. 
This is the return value of the `st_f` function described in the next bullet.


* `st_f` is an R function which calculates a summary statistic. It
must be defined using the three arguments named `i`, `xi` and `sdp`
in the stated order. The role of this function is to represent terms in the definition of record additivity
with each of the three arguments in `st_f` corresponding the the similarly spelled
terms in $t_i(x_i, s_{dp})$.

* `npar` is an integer value that represents the dimension of $\theta$. It 
should output a vector of the same length as `post_f`.


## Sampling

The main function in \pkg{dapper} is the `dapper_sample` function. The syntax of the function is:

```{r, echo = TRUE, eval = FALSE}
dapper_sample(data_model, sdp, init_par, niter = 2000, warmup = floor(niter / 2),
           chains = 1, varnames = NULL)
```

The three required inputs into `dapper_sample` function are the privacy model (`data_model`) whose construction is described in 4.1, the value
of the  observed privatized statistic (`sdp`), and the total number of observations 
in the complete data (`nobs`). The dapper
package is best suited for problems where the complete data can be represented in
tabular form. This is because internally, it is represented as a matrix.

The optional arguments are the number of mcmc draws (`niter`), the 
burn in period (`warmup`), number of chains (`chains`) and character
vector that names the parameters. Running multiple chains can be done in parallel
using the \CRANpkg{furrr} package. Additionally, progress can be monitored
using the \CRANpkg{progressr} package. Adhering to the design philosophy
of the two packages, we leave the setup to the user so that they may 
choose the most appropriate configuration for their system. The 
contingency table demonstration given in section 5 walks
through a typical setup of \CRANpkg{furrr} and \CRANpkg{progressr}.

The `dapper_sample` function returns a list containing
a `draw_matrix` and a vector of acceptance probabilities of size `niter`. The `draw_matrix` object is described
in more detail in the \CRANpkg{posterior} package. One of the advantages with working
with a `draw_matrix` object is that is compatible with many of the packages in
the \CRANpkg{rstan} ecosystem. For example, any `draw_matrix` object can be
plugged directly into the popular \CRANpkg{bayesplot} package. Additionaly,
`dapper`'s basic summary function provides the same posterior summary statistics
as those found when using \CRANpkg{rstan}. Overall, this should make working with `dapper` easier 
for anyone already familiar with the \CRANpkg{rstan} ecosystem.

# Examples

## 2x2 Contingency Table
As a demonstration, we analyze the UC Berkeley admissions data, which is often
used as an illustrative example of Simpson's paradox. The question posed is whether
the data suggest there is bias against females during the college admissions 
process. Below is a table of the aggregate admissions result from six departments based on sex
for a total of $N = 4526$ applicants. The table on the left represents
the true admissions data and the table on the right is the result of adding
independent, $N(0,100^2)$ error to each cell. Throughout the example
we assume we only have access to the original total count, $N$, and
the noise infused table.



```{r, echo = FALSE}
set.seed(1)
tmp <- apply(UCBAdmissions, 3, identity, simplify=FALSE)
adm_cnf <- Reduce('+', tmp)
adm_prv <- round(adm_cnf + rnorm(4, mean = 0, sd = 100), 2)

cnf_df <- tibble(gender = c("Male", "Male", "Female", "Female"),
                 status = c("Admitted","Rejected","Admitted","Rejected"),
                 n = c(1198, 1493, 557, 1278)) %>% uncount(n)

set.seed(1) 
ix <- sample(1:nrow(cnf_df), 400, replace = FALSE)
cnf_df <- cnf_df[ix,]
prv_df <- cnf_df
ip1 <- as.logical(rbinom(400, 1, .5))
ip2 <- as.logical(rbinom(400, 1, .5))
prv_df$gender[ip1] <- sample(c("Male", "Female"), sum(ip1), replace = TRUE)
prv_df$status[ip2] <- sample(c("Admitted", "Rejected"), sum(ip2), replace = TRUE)

adm_cnf <- table(cnf_df)
adm_prv <- table(prv_df)

v1 <- case_match(prv_df$gender, "Male" ~ 1, "Female"~ 0)
v2 <- case_match(prv_df$status, "Admitted" ~ 1, "Rejected"~ 0)

sdp <- c(v1, v2)
```


```{r, echo = FALSE}
kbl(list(t(adm_cnf), t(adm_prv)), booktabs = TRUE) %>%
  kable_styling(position = 'center', latex_options = c("hold_position"))
```

As mentioned in section 4, it is critical to choose a consistent
matrix representation for the confidential data. In this example
we represent the confidential admissions data as a $1 \times 4$ matrix.
Below we walk through the process of defining a privacy model.


1. `latent_f`: Since we can condition on the table total $N$, we can model the original, unobserved table counts as a multinomial
distribution. We can easily draw from this distribution using the
`rmultinom` function in the `base` stats package. Note, in this example,
the return value of one sample from `rmultinom` is a $4 \times 1$ matrix, so
in order to conform with our confidential data representation we take the transpose.
    ```{r, echo = TRUE}
    latent_f <- function(theta) {
      tl <- list(c(1,1), c(1,0), c(0,1), c(0,0))
      rs <- sample(tl, 400, replace = TRUE, prob = theta)
      do.call(rbind, rs)
    }
    ```
2. `post_f`: Given confidential data, we can derive the posterior analytically
using a Dirichlet prior. In this example, we use a flat prior which
corresponds to Dirch(1) distribution. A sample from the Dirichlet distribution
can be generated using random draws from the gamma distribution.
    ```{r, echo = TRUE}
     post_f <- function(dmat, theta) {
      x <- c(table(as.data.frame(dmat)))
      t1 <- rgamma(4, x + 1, 1)
      t1/sum(t1)
    }
    ```
3. `st_f`: Since the latent model only returns one observation from a multinomial
distribution, we can just use the identity function as the summary statistic.
    ```{r, echo = TRUE}
    st_f <- function(i, xi, sdp) {
      x <- rep(0, 400 * 2)
      
      x[i] <- xi[1]
      x[i + 400] <- xi[2]

      x
    }
    ```
4. `priv_f`: The privacy mechanism is Guassian white noise drawn from independent $N(0,100^2)$ distributions. Hence given
confidential table cells $(n_{11}, n_{22}, n_{12}, n_{21})$
\[
\eta(s_{dp} \mid x) = \prod \phi(s_{sd}; n_{ij}, 100^2).
\]
Here $\phi(\cdot;\mu,\sigma^2)$ is the density of the normal distribution
with mean and variance $\mu,\sigma^2$.
    ```{r, echo = TRUE}
    priv_f <- function(sdp, tx) {
      t1 <- sum(sdp == tx)
      t1 * log(3/4) + (800 - t1) * log(1/4)
    }
    ```


<!-- end of list -->

The `UBCAdmissions` data set is part of the `datasets` base package.
Below we load the data and create the noisy admissions table.

```{r, echo = TRUE, eval = FALSE}
set.seed(1)
tmp <- apply(UCBAdmissions, 3, identity, simplify=FALSE)
adm_cnf <- Reduce('+', tmp)
adm_prv <- round(adm_cnf + rnorm(4, mean = 0, sd = 100), 2)

x <- c(adm_cnf)
sdp <- c(adm_prv)
```




Once we have defined all components of the model we can
create a new privacy model object using the `new_privacy` function and
feed this into the `dapper_sample` function. Below we simulate 10,000 posterior
draws with a burn-in of 1000. 

```{r, echo = TRUE}
library(dapper)
library(furrr)
plan(multisession, workers = 4)

dmod <- new_privacy(post_f   = post_f,
                    latent_f = latent_f,
                    priv_f   = priv_f,
                    st_f     = st_f,
                    npar     = 4,
                    varnames = c("pi_11", "pi_21", "pi_12", "pi_22"))
                  
dp_out <- dapper_sample(dmod,
                  sdp = sdp,
                  niter = 5000,
                  warmup = 1000,
                  chains = 4,
                  init_par = rep(.25,4))
```

To run the `dapper_sample` function with parallel chains
we can import the `furrr` package and use the `plan` function
to determine the number of works used. The example code chunk below
would produce four chains using two CPU's.

```{r, eval = FALSE, echo = TRUE}
library(furrr)
library(progressr)
plan(multisession, workers = 2)
handlers(global = TRUE)
handlers("cli")


dp_out <- dapper_sample(dmod,
                  sdp = sdp,
                  niter = 5000,
                  warmup = 1000,
                  chains = 4,
                  init_par = rep(.25,4))
```

If the run time of `dapper_sample` is exceptionally long, one can
use the `progressr` package to monitor progress. The `progressor` framework
allows for a unified handling of progress bars in both the sequential and
parallel computing case.

```{r, eval = FALSE, echo = TRUE}
library(progressr)
handlers(global = TRUE)

handlers("cli")
dp_out <- dapper_sample(dmod,
                  sdp = c(adm_prv),
                  niter = 10000,
                  warmup = 100,
                  chains = 4,
                  init_par = rep(.25,4))
```

results can be quickly summarized using the `summary` function which is 
displayed below. The `rhat` values in the table are close to 1, which indicates
the chain has run long enough to achieve adequate mixing.

```{r,echo = FALSE}
summary(dp_out)
```

Diagnostic checks using trace plots can be done using the \pkg{Bayesplot} package
as shown in figure \@ref(fig:trace-plot). It is especially important to check for good mixing
with `dapper` since sticky chains are likely to be produced
when the amount of injected noise is high. See the discussion section
for a more detailed explanation. 

```{r trace-plot,  fig.cap="trace plots.", fig.height=3, fig.width=5, fig.align='center'}
plot(dp_out)
```

To see if there is evidence of gender bias we can look at the odds ratio.
Specifically, we look at the odds of a male being admitted to
that of female. A higher odds ratio would indicate a bias
favoring males. Figure \@ref(fig:post-or-density) shows the posterior draws
from the dapper model. The large odds ratio values would seem
to indicate there is bias favoring the males. See [@Bickel1975] for
an explanation of the "paradox" of this result.

```{r post-or-density, fig.cap="posterior density estimate for the odds ratio using 9000 MCMC draws.",  fig.height=3, fig.width=5, fig.align='center'}
tv <- dp_out$chain
or <- as.numeric((tv[,1] * tv[,4]) / (tv[,2] * tv[,3]))
ggplot(tibble(x=or), aes(x)) + geom_density() + xlim(0,10) + xlab("Odds Ratio")
```



For comparison, we run a standard Bayesian analysis on the 
noise infused table ignoring the privacy mechanism. This will
correspond exactly to the model defined in the `post_f` component.
Figure \@ref(fig:post-or-compare) shows a density estimate for the odds ratio
under the confidential and noisy data. The posterior
distribution for the odds ratio under the noisy data
is shifted significantly, indicating a large degree of bias.
Looking at left hand plot in figure \@ref(fig:post-or-compare) shows the MAP estimate from `dapper`
is similar to that in the case of the confidential data.
The width of the posterior is also much larger since
it properly accounts for the uncertainty due to the privacy mechanism. This
illustrates the dangers of ignoring the privacy mechanism: a naive
analysis not only has bias, but also severely underestimates the 
uncertainty associated with the odds ratio estimate.



```{r post-or-compare, fig.cap= caption,  echo = FALSE, fig.height=3, fig.width=5, fig.align='center'}
caption <- "Comparison between using dapper and a naive Bayesian anaylsis on the
noise infused data and the original confidential data." 

set.seed(1)
x <- cnf_df
x$gender <- case_match(cnf_df$gender, "Male" ~ 1, "Female"~ 0)
x$status <- case_match(cnf_df$status, "Admitted" ~ 1, "Rejected"~ 0)
confidential_data <- x
cps <- t(sapply(1:16000, function(s) post_f(confidential_data, NULL)))
odds_male   <- cps[,1] / cps[,2]
odds_female <- cps[,3] / cps[,4]
odds_ratio_conf  <- odds_male/odds_female

set.seed(1)
noisy_data <- matrix(sdp, ncol = 2, byrow = FALSE)
cps <- t(sapply(1:16000, function(s) post_f(noisy_data, NULL)))
odds_male   <- cps[,1] / cps[,2]
odds_female <- cps[,3] / cps[,4]
odds_ratio_noisy  <- odds_male/odds_female

df1 <- tibble(confidential = odds_ratio_conf, noisy = odds_ratio_noisy) %>%
  pivot_longer(everything(), names_to = "group", values_to = "odds_ratio") %>%
  mutate(method = "naive")

df2 <- tibble(confidential = odds_ratio_conf, noisy = or) %>%
  pivot_longer(everything(), names_to = "group", values_to = "odds_ratio") %>%
  mutate(method = "dapper")

df <- rbind(df1,df2)

df %>%  ggplot(aes(odds_ratio, group = group, fill = group)) + 
  geom_density(alpha = .5) + 
  facet_wrap(~method) + 
  xlim(0,8) +
  xlab("Odds Ratio") +
  guides(fill= guide_legend(title= "Data")) + 
  theme(legend.position="bottom")
```




## Linear Regression
In this section we apply `dapper` to reconstruct an
example presented in @Ju2022. They apply a Laplace privacy 
mechanism to a sufficient summary statistic for a linear regression model.
Let $\{(x_i,y_i)\}_{i=1}^{n}$ be the original, confidential data with $x_i \in \mathbb{R}^2$.
They assume the true data generating process follows the model

\[
\begin{aligned}
y &= -1.79 -2.89x_1 -0.66x_2 + \epsilon\\
\epsilon &\sim N(0,2^2)\\
\binom{x_1}{x_2} &\sim N_{2}(\mu, I_2)\\
\mu &= \binom{0.9}{-1.17}.
\end{aligned}
\]

Note, in most settings involving linear regression, the covariates are assumed to be
fixed, known constants. Thus the formulation above is a departure from the norm since
we are assuming a random design matrix. More details on why this framing is necessary 
will be provided later when describing the latent model.

The paper considers the scenario where one desires to publicly release the
sufficient summary statistics

\[
s(x,y) = (x^Ty, y^Ty, x^Tx).
\]

This summary statistic satisfies the additive record property since $s(x,y) = \sum_{i=1}^{n} t(x_i, y_i)$ 
where

\[
t(x_i,y_i) = ((x_{i})^T y_i, y_i^2, (x_{i})^T x_i),
\]



with $\epsilon$-DP privacy guarantees. To achieve this guarantee it is 
necessary to bound the value of the statistic. This will ensure
a data point cannot be too "unique." Intuitively, an outlier is 
easy to identify, so in essence we want to make sure no data point
is an extreme outlier. In the language of differential
privacy, this equates to bounding the global sensitivity. This was accomplished
by clamping the data. More precisely, we define 
the clamp function $[z] := \min\{\max\{z,-10\}, 10\}$ which truncates a value
$z$ so that it falls into the interval $[-10,10]$. Furthermore, we let $\tilde{z} := [z]/10$
denote the normalized clamped value of $z$. The clamped statistic is

\[
t(x_i,y_i) = ((\tilde{x}^{i})^T \tilde{y}_i, \tilde{y}_i^2, (\tilde{x}_{i})^T \tilde{x}_i).
\]

Ignoring duplicate entries, the global sensitivity is $\Delta = p^2 + 4p + 3$ ^[The original paper, @Ju2022, contains a computation error and uses $\Delta = p^2 + 3p + 3$].
Using the Laplace mechanism, $\epsilon$-DP privacy can thus be achieved by adding i.i.d. Laplace$(0, \Delta/\epsilon)$
error to each unique entry. Note this only provides an upper bound on sensitivity, and a 
tighter can be achieved using techniques described in [@Awan2020].

1. `latent_f`: Since the privacy mechanism involves injecting noise into the design
matrix, it is not possible to use the standard approach where one assumes the design
matrix is a fixed, known constant. Hence to draw a sample from the latent data generating 
process we use the relation $f(x,y) = f(x)f(y \mid x)$. In this formulation,
it is necessary to specify a distribution on the covariates $x$.
    ```{r, echo = TRUE}
    latent_f <- function(theta) {
      xmat <- MASS::mvrnorm(50 , mu = c(.9,-1.17), Sigma = diag(2))
      y <- cbind(1,xmat) %*% theta + rnorm(50, sd = sqrt(2))
      cbind(y,xmat)
    }
    ```
2. `post_f`: Given confidential data $X$ we can derive the posterior analytically
using a normal prior on $\beta$.
\[
\begin{aligned}
\beta &\sim N_{p+1}(0, \tau^2 I_{p+1})\\
\beta \mid x,y &\sim N(\mu_n, \Sigma_n)\\
\Sigma_n &= (x^Tx/\sigma^2 + I_{p+1}/\tau^2)^{-1}\\
\mu_n &= \Sigma_n(x^Ty)/\sigma^2
\end{aligned}
\]
In the example, we use $\sigma^2 = 2$ and $\tau^2 = 4$.
    ```{r, echo = TRUE}
    post_f <- function(dmat, theta) {
      x <- cbind(1,dmat[,-1])
      y <- dmat[,1]

      ps_s2 <- solve((1/2) * t(x) %*% x + (1/4) * diag(3))
      ps_m <- ps_s2 %*% (t(x) %*% y) * (1/2)

      MASS::mvrnorm(1, mu = ps_m, Sigma = ps_s2)
    }
    ```
3. `st_f`: The summary statistic contains duplicate
entries. We can considerable reduce the dimension of the 
statistic by only considering unique entries. The `clamp_data`
function is used to bound the statistic to give a finite
global sensitivity.
    ```{r, echo = TRUE}
    clamp_data <- function(dmat) {
      pmin(pmax(dmat,-10),10) / 10
    }
    
    st_f <- function(i, tx, sdp) {
      txc <- clamp_data(tx)
      ydp <- txc[1]
      xdp <- cbind(1,t(txc[-1]))
        
      s1 <- t(xdp) %*% ydp
      s2 <- t(ydp) %*% ydp
      s3 <- t(xdp) %*% xdp
    
      ur_s1 <- c(s1)
      ur_s2 <- c(s2)
      ur_s3 <- s3[upper.tri(s3,diag = TRUE)][-1]
      c(ur_s1,ur_s2,ur_s3)
    }
    ```
4. `priv_f`: Privacy Mechanism
adds Laplace$(0, \Delta/\epsilon)$ error to each unique entry
of the statistic. In this example, $\Delta = 15$ and $\epsilon = 10$.
    ```{r, echo = TRUE}
    priv_f <- function(sdp, zt) {
      sum(VGAM::dlaplace(sdp - zt, 0, 15/10, log = TRUE))
    }
    ```

First we simulate fake data using the aforementioned privacy mechanism.
In the example, we use $n = 50$ observations.

```{r, echo = TRUE}
deltaa <- 15
epsilon <- 10
n <- 50

set.seed(1)
xmat <- MASS::mvrnorm(n, mu = c(.9,-1.17), Sigma = diag(2))
beta <- c(-1.79, -2.89, -0.66)
y <- cbind(1,xmat) %*% beta + rnorm(n, sd = sqrt(2))

#clamp the confidential data in xmat
dmat <- cbind(y,xmat)
sdp <-  apply(sapply(1:nrow(dmat), function(i) st_f(i, dmat[i,], sdp)), 1, sum)

#add Laplace noise 
sdp <- sdp + VGAM::rlaplace(length(sdp), location = 0, scale = deltaa/epsilon)
```

We construct a privacy model using the `new_privacy` function and
make 25,000 MCMC draws with a burn in of 1000 draws.

```{r, echo = TRUE, cache = TRUE}
library(dapper)

dmod <- new_privacy(post_f   = post_f,
                    latent_f = latent_f,
                    priv_f   = priv_f,
                    st_f     = st_f,
                    npar     = 3,
                    varnames = c("beta0", "beta1", "beta2"))



dp_out <- dapper_sample(dmod,
                        sdp = sdp,
                        niter = 25000,
                        warmup = 1000,
                        chains = 1,
                        init_par = rep(0,3))

```


For comparison, we consider a Bayesian analysis where the design matrix
is a fixed known constant and $\sigma^2$ is known. Using the
diffuse prior $f(\beta) \propto 1$ leads to normal posterior.
\[
\begin{aligned}
f(\beta \mid x,y, \sigma^2) &\sim N(\hat{\beta}, \hat{\Sigma})\\
\hat{\mu} &= (x^Tx)^{-1}xy\\
\hat{\Sigma} &= \sigma^{2}(x^Tx)^{-1}
\end{aligned}
\]

The posterior can be written as a function of $s(x,y)$. Since
we only have access to the noisy version $s_{dp}$ we can
attempt to reconstruct the posterior be extracting the
relevant entries which is done below.

```{r, echo = FALSE}
#x^Ty
s1 <- sdp[1:3]

#y^Ty
s2 <- sdp[4]

#x^Tx
s3 <- matrix(0, nrow = 3, ncol = 3)
s3[upper.tri(s3, diag = TRUE)] <- c(n, sdp[5:9])
s3[lower.tri(s3)] <- s3[upper.tri(s3)]
```

Because of the injected privacy noise, the reconstructed
$(x^Tx)^{-1}$ matrix is not positive definite. As
a naive solution we use the algorithm proposed  
in [@Higham1988] to find the closest positive semi-definite matrix
as determined by the Forbenius norm. The `pracma`
package contains an implementation via the `nearest_psd`
function.

```{r, echo = FALSE}
s3 <- pracma::nearest_spd(solve(s3))
bhat <- s3 %*% s1
sigma_hat <- 2^2 * s3
```

Figure \@ref(fig:regression-compare) shows the posterior density estimates for the $\beta$ coefficients based
on $s_{dp}$. The density estimates indicates the naive method, which ignores the privacy mechanism, has bias and 
underestimates the variance. Likewise Figure \@ref(fig:regression-data-compare) 
illustrates how dapper provides point estimates that are not far off from those
that would have been obtained using the original confidential data. And the dramatic
increase in the posterior variance indicates the privacy mechanism adds substantial
uncertainty to the estimates.


```{r regression-compare, fig.cap = caption, echo = FALSE, fig.height=3, fig.width=5, fig.align='center'}
caption <- "Comparison between dapper and a naive approach that ignores the privacy mechanism. The dashed lines
are the true coefficient values."

coef_df <- dp_out$chain %>% 
  as_tibble() %>%
  pivot_longer(contains("beta"), names_to = "coefficient", values_to = "estimate") %>%
  mutate(method = "dapper")

coef_med <- coef_df %>% 
  group_by(coefficient) %>%
  summarise(median = median(estimate))

coef_true <- tibble(coefficient = c("beta0", "beta1", "beta2"),
                    value  = c(-1.79, -2.89, -0.66))

naive_sample <- MASS::mvrnorm(9000, mu = bhat, Sigma = sigma_hat)

coef_post <- tibble(beta0 = naive_sample[,1],
                    beta1 = naive_sample[,2],
                    beta2 = naive_sample[,3])

coef_post <- coef_post %>% 
  pivot_longer(contains("beta"), names_to = "coefficient", values_to = "estimate") %>%
  mutate(method = "naive")

rbind(coef_df, coef_post) %>%
  ggplot(aes(x = estimate, group = method, fill = method)) + geom_density(alpha = .5) + 
  geom_vline(aes(xintercept = value), data = coef_true, linetype = "dashed") +
  facet_wrap(~coefficient, scale = 'free') + coord_cartesian(ylim=c(0, 0.7)) +
  theme(legend.position="bottom")
```


```{r regression-data-compare, fig.cap=caption, echo = FALSE, fig.height=3, fig.width=5, fig.align='center'}
caption <- "Comparison between using dapper on the noisy data set and a standard
Bayesian analysis on the confidential data set."
coef_df <- dp_out$chain %>% 
  as_tibble() %>%
  pivot_longer(contains("beta"), names_to = "coefficient", values_to = "estimate") %>%
  mutate(data = "noisy")

xmt <- cbind(1,xmat)
conf_bhat <- solve(t(xmt) %*% xmt) %*% (t(xmt) %*% y)
conf_sigma_hat <- 2^2 * solve(t(xmt) %*% xmt)
conf_sample <- MASS::mvrnorm(9000, mu = conf_bhat, Sigma = conf_sigma_hat)


coef_post <- tibble(beta0 = conf_sample[,1],
                    beta1 = conf_sample[,2],
                    beta2 = conf_sample[,3])

coef_post <- coef_post %>% 
  pivot_longer(contains("beta"), names_to = "coefficient", values_to = "estimate") %>%
  mutate(data = "confidential")

rbind(coef_df, coef_post) %>%
  mutate(data = as_factor(data)) %>% 
  ggplot(aes(x = estimate, group = data, fill = data)) + geom_density(alpha = .5) + 
  geom_vline(aes(xintercept = value), data = coef_true, linetype = "dashed") +
  facet_wrap(~coefficient, scale = 'free') + coord_cartesian(ylim=c(0, 0.7)) +
  theme(legend.position="bottom")

```


# Discussion
## Poor Mixing
Mixing can be poor when the posterior under a given privacy mechanism is 
much wider than the posterior that would arise using the confidential data. In
other words, when the privacy budget is small, poor mixing can be expected. The
rest of this section explores a toy example that will provide insight into this
phenomenon.

Suppose the confidential data consist of a single observation $x \in \mathbb{R}$,
and consider the scenario where a user makes a request to view $x$ and
in return receives $s := x + \nu$, which is a noise infused version of $x$.
For simplicity, we do not worry about constructing an $\epsilon$-DP privacy mechanism, 
and take $\nu \sim N(0, \epsilon^{-2})$ for some $\epsilon > 0$. However, it
will still be useful to think of $\epsilon$ as the privacy budget since smaller values
of $\epsilon$ correspond to a larger amounts of noise. Using
a flat prior and a normally distributed likelihood results in
a normally distributed posterior described below.

\[
\begin{aligned}
f(\theta) &\propto 1\\
s \mid x &\sim N(x, \epsilon^{-2})\\
x \mid \theta &\sim N(\theta, \sigma^2)\\
\end{aligned}
\]

With the above model, the data augmentation process consist of the 
two steps

+  Step 1: Sample from $x \mid \theta, s \sim N(\mu, \tau^2)$, where
$\mu$ and $\tau$ are defined as:
\[
\begin{aligned}
\mu &:= \dfrac{\dfrac{1}{\epsilon^{-2}}s + \dfrac{1}{\sigma^2}\theta}{\dfrac{1}{\epsilon^{-2}} + \dfrac{1}{\sigma^2}}\\
\tau^2 &:= \dfrac{1}{\dfrac{1}{\epsilon^{-2}} + \dfrac{1}{\sigma^2}}.
\end{aligned}
\]

+ Step 2: Sample from $\theta \mid x, s  \sim N(x, \sigma^2)$.

It turns out the lag-1 auto-correlation is closely related
to the convergence rate of the chain in this example. [@Liu1999] showed
the lag-1 autocorrelation is related to the Bayesian fraction of missing information
and the latter, in fact, gives the exact convergence rate. The Bayesian fraction of missing information, $\gamma$ is
defined as
\begin{align*}
\gamma &:= 1 - \dfrac{E[Var(\theta \mid s, x) \mid s]}{Var(\theta \mid s)} = 1 - \dfrac{E[Var(\theta \mid x)]}{Var(\theta \mid s)}.
\end{align*}

plugging in the appropriate quantities gives us 

\begin{align*}
\gamma &= 1 - \dfrac{\sigma^2}{\sigma^2 + \epsilon^{-2}} = 1 - \dfrac{1}{1 + \dfrac{\epsilon^{-2}}{\sigma^2}}.
\end{align*}

The chain converges faster as $\gamma \to 0$ an slower as $\gamma \to 1$.
From the right hand term in the above panel, we can see $\gamma$
depends only on $\epsilon^{-2}/\sigma^2$ and as the privacy budget decreases (i.e. more noise is being added to $x$),
$\gamma \to 1$.

Thus when poor mixing is observed, we recommend seeing if increasing the privacy
budget helps. Unfortunately, if one is not at liberty to alter the privacy budget, 
there is no quick fix, and other sampling scheme may be needed.
In the case where a small privacy budget results
in a posterior much more diffuse than under the confidential data set, one
can draw samples using the pseudo-likelihood scheme as proposed in [@Andrieu2009]. This
scheme fits in the same data augmentation framework as `dapper` but is not implemented.

## Related Work
Adjusting statistical workflows to account for added noise in covariates
is extensively studied in the context of measurement error (e.g. noisy sensor readings)
and there exists readily available tools for making the proper adjustments.
For textbook length treatments on the topic see [@Yi2017; @Carroll2006].
Work in this area mostly focuses on methods which do not require fully specifying the 
measurement error model, since this is often assumed unknown.
However, in differential privacy, the measurement error model is exactly known.
This difference, makes feasible some ideas which the measurement
error community has not previously considered [@Smith2011; @Karwa2015].



# Summary

Currently, there is a lack of software tools privacy researchers can use
to evaluate the impact of privacy mechanisms on statistical analyses.
While there have been tremendous gains in the theoretical aspects of privacy,
the lack of software resources to deploy and work with new privacy techniques has
hampered their adoption. This gap in capability has been noted by several
large industry entities who have begun building software ecosystems for
working with differential privacy. SmartNoise by Microsoft [insert citation] and SafeTab [insert citation] by the US Census, for example,
are tools for generating synthetic data that has differentially private guarantees.
However, the majority of these software tools only address privacy and not the 
ensuing analysis or, if it does, address the analysis only for specific models.
Privacy researchers currently lack good tools for evaluating the impact
of privacy mechanisms on a statistical analysis.

Thus `dapper` helps fill an urgent need by providing researchers a way to properly account
for the noise introduced for privacy protection in their statistical analysis. A notable
feature is its flexibility which allows the users to specify a custom
privacy mechanism. The benefit being that `dapper` can evaluate already
established privacy mechanisms and those that have yet to be discovered.


`dapper` offers a significant step forward in providing general-purpose statistical
inference tools for privatized data. Despite the strengths of `dapper`, there
are notable weaknesses, which highlight the important directions for future work.
Weaknesses include the requirement for the privacy mechanism to have a density 
(e.g. the exponential mechanism has no density). The non-private posterior needs
a relatively efficient sampler. Slow convergence when the privacy budget is small,
and a statistic that satisfies record additivity to fully leverage `dapper` computational
potential.



